--- a/drivers/gpu/drm/i915/i915_dma.c
+++ b/drivers/gpu/drm/i915/i915_dma.c
@@ -1648,7 +1648,11 @@ int i915_driver_load(struct drm_device *
 	return 0;
 
 out_gem_unload:
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3,12,0))
 	if (dev_priv->mm.inactive_shrinker.scan_objects)
+#else
+	if (dev_priv->mm.inactive_shrinker.shrink)
+#endif
 		unregister_shrinker(&dev_priv->mm.inactive_shrinker);
 
 	if (dev->pdev->msi_enabled)
@@ -1682,7 +1686,11 @@ int i915_driver_unload(struct drm_device
 
 	i915_teardown_sysfs(dev);
 
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3,12,0))
 	if (dev_priv->mm.inactive_shrinker.scan_objects)
+#else
+	if (dev_priv->mm.inactive_shrinker.shrink)
+#endif
 		unregister_shrinker(&dev_priv->mm.inactive_shrinker);
 
 	mutex_lock(&dev->struct_mutex);
--- a/drivers/gpu/drm/i915/i915_gem.c
+++ b/drivers/gpu/drm/i915/i915_gem.c
@@ -56,10 +56,15 @@ static void i915_gem_object_update_fence
 					 struct drm_i915_fence_reg *fence,
 					 bool enable);
 
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3,12,0))
 static unsigned long i915_gem_inactive_count(struct shrinker *shrinker,
 					     struct shrink_control *sc);
 static unsigned long i915_gem_inactive_scan(struct shrinker *shrinker,
 					    struct shrink_control *sc);
+#else
+static int i915_gem_inactive_shrink(struct shrinker *shrinker,
+				    struct shrink_control *sc);
+#endif
 static long i915_gem_purge(struct drm_i915_private *dev_priv, long target);
 static long i915_gem_shrink_all(struct drm_i915_private *dev_priv);
 static void i915_gem_object_truncate(struct drm_i915_gem_object *obj);
@@ -4464,8 +4469,12 @@ i915_gem_load(struct drm_device *dev)
 
 	dev_priv->mm.interruptible = true;
 
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3,12,0))
 	dev_priv->mm.inactive_shrinker.scan_objects = i915_gem_inactive_scan;
 	dev_priv->mm.inactive_shrinker.count_objects = i915_gem_inactive_count;
+#else
+	dev_priv->mm.inactive_shrinker.shrink = i915_gem_inactive_shrink;
+#endif
 	dev_priv->mm.inactive_shrinker.seeks = DEFAULT_SEEKS;
 	register_shrinker(&dev_priv->mm.inactive_shrinker);
 }
@@ -4688,8 +4697,14 @@ static bool mutex_is_locked_by(struct mu
 #endif
 }
 
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3,12,0))
 static unsigned long
 i915_gem_inactive_count(struct shrinker *shrinker, struct shrink_control *sc)
+#else
+#define SHRINK_STOP -1
+static int
+i915_gem_inactive_shrink(struct shrinker *shrinker, struct shrink_control *sc)
+#endif
 {
 	struct drm_i915_private *dev_priv =
 		container_of(shrinker,
@@ -4698,7 +4713,12 @@ i915_gem_inactive_count(struct shrinker
 	struct drm_device *dev = dev_priv->dev;
 	struct drm_i915_gem_object *obj;
 	bool unlock = true;
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3,12,0))
 	unsigned long count;
+#else
+	int nr_to_scan = sc->nr_to_scan;
+	int count;
+#endif
 
 	if (!mutex_trylock(&dev->struct_mutex)) {
 		if (!mutex_is_locked_by(&dev->struct_mutex, current))
@@ -4710,6 +4730,17 @@ i915_gem_inactive_count(struct shrinker
 		unlock = false;
 	}
 
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,12,0))
+	if (nr_to_scan) {
+		nr_to_scan -= i915_gem_purge(dev_priv, nr_to_scan);
+		if (nr_to_scan > 0)
+			nr_to_scan -= __i915_gem_shrink(dev_priv, nr_to_scan,
+							false);
+		if (nr_to_scan > 0)
+			i915_gem_shrink_all(dev_priv);
+	}
+#endif
+
 	count = 0;
 	list_for_each_entry(obj, &dev_priv->mm.unbound_list, global_list)
 		if (obj->pages_pin_count == 0)
@@ -4800,6 +4831,7 @@ struct i915_vma *i915_gem_obj_to_vma(str
 	return NULL;
 }
 
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3,12,0))
 static unsigned long
 i915_gem_inactive_scan(struct shrinker *shrinker, struct shrink_control *sc)
 {
@@ -4833,3 +4865,4 @@ i915_gem_inactive_scan(struct shrinker *
 		mutex_unlock(&dev->struct_mutex);
 	return freed;
 }
+#endif
